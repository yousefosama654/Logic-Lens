{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04a0dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "\n",
    "#import commonfunctions as cf\n",
    "from skimage.feature import hog\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "import pickle\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75e3e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_b_dataset = r'letters_data_set/B'\n",
    "path_to_e_dataset = r'letters_data_set/E'\n",
    "path_to_h_dataset = r'letters_data_set/H'\n",
    "path_to_y_dataset = r'letters_data_set/Y'\n",
    "path_to_xor_dataset = r'letters_data_set/XOR'\n",
    "path_to_and_dataset = r'letters_data_set/AND'\n",
    "path_to_or_dataset = r'letters_data_set/OR'\n",
    "path_to_not_dataset = r'letters_data_set/NOT'\n",
    "path_to_b1_dataset = r'letters_data_set/b1'\n",
    "path_to_b2_dataset = r'letters_data_set/b2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2281e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img): \n",
    "    img=cv2.resize(img,(64,64))\n",
    "    fd, hog_image = hog(\n",
    "        img,\n",
    "        pixels_per_cell=(2, 2),\n",
    "        cells_per_block=(2, 2),\n",
    "        visualize=True,        \n",
    "    )\n",
    "    return fd,hog_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91799cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' 'c']\n"
     ]
    }
   ],
   "source": [
    "fd=np.array([1,2,3])\n",
    "fd=np.append(fd,'c')\n",
    "print(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "430e9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_filenames_b = os.listdir(path_to_b_dataset)\n",
    "    img_filenames_e = os.listdir(path_to_e_dataset)\n",
    "    img_filenames_h = os.listdir(path_to_h_dataset)\n",
    "    img_filenames_y = os.listdir(path_to_y_dataset)\n",
    "    img_filenames_and = os.listdir(path_to_and_dataset)\n",
    "    img_filenames_xor = os.listdir(path_to_xor_dataset)\n",
    "    img_filenames_not = os.listdir(path_to_not_dataset)\n",
    "    img_filenames_or = os.listdir(path_to_or_dataset)\n",
    "    img_filenames_b1 = os.listdir(path_to_b1_dataset)\n",
    "    img_filenames_b2 = os.listdir(path_to_b2_dataset)\n",
    "\n",
    "    for i, fn in enumerate(img_filenames_b):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 0\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_b_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)\n",
    "        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_b)))\n",
    "    print(\"B dataset processing done\")\n",
    "    for i, fn in enumerate(img_filenames_e):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 1\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_e_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_e)))     \n",
    "    print(\"E dataset processing done\")   \n",
    "    \n",
    "    for i, fn in enumerate(img_filenames_h):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 2\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_h_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_h)))     \n",
    "    print(\"H dataset processing done\")   \n",
    "    \n",
    "\n",
    "    for i, fn in enumerate(img_filenames_y):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 3\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_y_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_y)))     \n",
    "    print(\"Y dataset processing done\")   \n",
    "      \n",
    "    for i, fn in enumerate(img_filenames_xor):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 4\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_xor_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_xor)))     \n",
    "    print(\"XOR dataset processing done\")   \n",
    "      \n",
    "    for i, fn in enumerate(img_filenames_and):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 5\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_and_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_and)))     \n",
    "    print(\"AND dataset processing done\")  \n",
    "    for i, fn in enumerate(img_filenames_not):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 6\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_not_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_not)))     \n",
    "    print(\"NOT dataset processing done\")  \n",
    "    for i, fn in enumerate(img_filenames_or):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 7\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_or_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_or)))     \n",
    "    print(\"OR dataset processing done\")   \n",
    "    for i, fn in enumerate(img_filenames_b1):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 8\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_b1_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_b1)))     \n",
    "    print(\"( dataset processing done\")   \n",
    "    for i, fn in enumerate(img_filenames_b2):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 9\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_b2_dataset, fn)\n",
    "        img = io.imread(path)\n",
    "        fd,img = extract_hog_features(img)\n",
    "        fd=np.append(fd,label)\n",
    "        features.append(fd)        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_b2)))     \n",
    "    print(\") dataset processing done\")   \n",
    "    \n",
    "    \n",
    "\n",
    "    return features, labels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1adad80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10/155\n",
      "[INFO] processed 20/155\n",
      "[INFO] processed 30/155\n",
      "[INFO] processed 40/155\n",
      "[INFO] processed 50/155\n",
      "[INFO] processed 60/155\n",
      "[INFO] processed 70/155\n",
      "[INFO] processed 80/155\n",
      "[INFO] processed 90/155\n",
      "[INFO] processed 100/155\n",
      "[INFO] processed 110/155\n",
      "[INFO] processed 120/155\n",
      "[INFO] processed 130/155\n",
      "[INFO] processed 140/155\n",
      "[INFO] processed 150/155\n",
      "B dataset processing done\n",
      "[INFO] processed 10/152\n",
      "[INFO] processed 20/152\n",
      "[INFO] processed 30/152\n",
      "[INFO] processed 40/152\n",
      "[INFO] processed 50/152\n",
      "[INFO] processed 60/152\n",
      "[INFO] processed 70/152\n",
      "[INFO] processed 80/152\n",
      "[INFO] processed 90/152\n",
      "[INFO] processed 100/152\n",
      "[INFO] processed 110/152\n",
      "[INFO] processed 120/152\n",
      "[INFO] processed 130/152\n",
      "[INFO] processed 140/152\n",
      "[INFO] processed 150/152\n",
      "E dataset processing done\n",
      "[INFO] processed 10/229\n",
      "[INFO] processed 20/229\n",
      "[INFO] processed 30/229\n",
      "[INFO] processed 40/229\n",
      "[INFO] processed 50/229\n",
      "[INFO] processed 60/229\n",
      "[INFO] processed 70/229\n",
      "[INFO] processed 80/229\n",
      "[INFO] processed 90/229\n",
      "[INFO] processed 100/229\n",
      "[INFO] processed 110/229\n",
      "[INFO] processed 120/229\n",
      "[INFO] processed 130/229\n",
      "[INFO] processed 140/229\n",
      "[INFO] processed 150/229\n",
      "[INFO] processed 160/229\n",
      "[INFO] processed 170/229\n",
      "[INFO] processed 180/229\n",
      "[INFO] processed 190/229\n",
      "[INFO] processed 200/229\n",
      "[INFO] processed 210/229\n",
      "[INFO] processed 220/229\n",
      "H dataset processing done\n",
      "[INFO] processed 10/287\n",
      "[INFO] processed 20/287\n",
      "[INFO] processed 30/287\n",
      "[INFO] processed 40/287\n",
      "[INFO] processed 50/287\n",
      "[INFO] processed 60/287\n",
      "[INFO] processed 70/287\n",
      "[INFO] processed 80/287\n",
      "[INFO] processed 90/287\n",
      "[INFO] processed 100/287\n",
      "[INFO] processed 110/287\n",
      "[INFO] processed 120/287\n",
      "[INFO] processed 130/287\n",
      "[INFO] processed 140/287\n",
      "[INFO] processed 150/287\n",
      "[INFO] processed 160/287\n",
      "[INFO] processed 170/287\n",
      "[INFO] processed 180/287\n",
      "[INFO] processed 190/287\n",
      "[INFO] processed 200/287\n",
      "[INFO] processed 210/287\n",
      "[INFO] processed 220/287\n",
      "[INFO] processed 230/287\n",
      "[INFO] processed 240/287\n",
      "[INFO] processed 250/287\n",
      "[INFO] processed 260/287\n",
      "[INFO] processed 270/287\n",
      "[INFO] processed 280/287\n",
      "Y dataset processing done\n",
      "[INFO] processed 10/272\n",
      "[INFO] processed 20/272\n",
      "[INFO] processed 30/272\n",
      "[INFO] processed 40/272\n",
      "[INFO] processed 50/272\n",
      "[INFO] processed 60/272\n",
      "[INFO] processed 70/272\n",
      "[INFO] processed 80/272\n",
      "[INFO] processed 90/272\n",
      "[INFO] processed 100/272\n",
      "[INFO] processed 110/272\n",
      "[INFO] processed 120/272\n",
      "[INFO] processed 130/272\n",
      "[INFO] processed 140/272\n",
      "[INFO] processed 150/272\n",
      "[INFO] processed 160/272\n",
      "[INFO] processed 170/272\n",
      "[INFO] processed 180/272\n",
      "[INFO] processed 190/272\n",
      "[INFO] processed 200/272\n",
      "[INFO] processed 210/272\n",
      "[INFO] processed 220/272\n",
      "[INFO] processed 230/272\n",
      "[INFO] processed 240/272\n",
      "[INFO] processed 250/272\n",
      "[INFO] processed 260/272\n",
      "[INFO] processed 270/272\n",
      "XOR dataset processing done\n",
      "[INFO] processed 10/240\n",
      "[INFO] processed 20/240\n",
      "[INFO] processed 30/240\n",
      "[INFO] processed 40/240\n",
      "[INFO] processed 50/240\n",
      "[INFO] processed 60/240\n",
      "[INFO] processed 70/240\n",
      "[INFO] processed 80/240\n",
      "[INFO] processed 90/240\n",
      "[INFO] processed 100/240\n",
      "[INFO] processed 110/240\n",
      "[INFO] processed 120/240\n",
      "[INFO] processed 130/240\n",
      "[INFO] processed 140/240\n",
      "[INFO] processed 150/240\n",
      "[INFO] processed 160/240\n",
      "[INFO] processed 170/240\n",
      "[INFO] processed 180/240\n",
      "[INFO] processed 190/240\n",
      "[INFO] processed 200/240\n",
      "[INFO] processed 210/240\n",
      "[INFO] processed 220/240\n",
      "[INFO] processed 230/240\n",
      "AND dataset processing done\n",
      "[INFO] processed 10/188\n",
      "[INFO] processed 20/188\n",
      "[INFO] processed 30/188\n",
      "[INFO] processed 40/188\n",
      "[INFO] processed 50/188\n",
      "[INFO] processed 60/188\n",
      "[INFO] processed 70/188\n",
      "[INFO] processed 80/188\n",
      "[INFO] processed 90/188\n",
      "[INFO] processed 100/188\n",
      "[INFO] processed 110/188\n",
      "[INFO] processed 120/188\n",
      "[INFO] processed 130/188\n",
      "[INFO] processed 140/188\n",
      "[INFO] processed 150/188\n",
      "[INFO] processed 160/188\n",
      "[INFO] processed 170/188\n",
      "[INFO] processed 180/188\n",
      "NOT dataset processing done\n",
      "[INFO] processed 10/215\n",
      "[INFO] processed 20/215\n",
      "[INFO] processed 30/215\n",
      "[INFO] processed 40/215\n",
      "[INFO] processed 50/215\n",
      "[INFO] processed 60/215\n",
      "[INFO] processed 70/215\n",
      "[INFO] processed 80/215\n",
      "[INFO] processed 90/215\n",
      "[INFO] processed 100/215\n",
      "[INFO] processed 110/215\n",
      "[INFO] processed 120/215\n",
      "[INFO] processed 130/215\n",
      "[INFO] processed 140/215\n",
      "[INFO] processed 150/215\n",
      "[INFO] processed 160/215\n",
      "[INFO] processed 170/215\n",
      "[INFO] processed 180/215\n",
      "[INFO] processed 190/215\n",
      "[INFO] processed 200/215\n",
      "[INFO] processed 210/215\n",
      "OR dataset processing done\n",
      "[INFO] processed 10/125\n",
      "[INFO] processed 20/125\n",
      "[INFO] processed 30/125\n",
      "[INFO] processed 40/125\n",
      "[INFO] processed 50/125\n",
      "[INFO] processed 60/125\n",
      "[INFO] processed 70/125\n",
      "[INFO] processed 80/125\n",
      "[INFO] processed 90/125\n",
      "[INFO] processed 100/125\n",
      "[INFO] processed 110/125\n",
      "[INFO] processed 120/125\n",
      "( dataset processing done\n",
      "[INFO] processed 10/136\n",
      "[INFO] processed 20/136\n",
      "[INFO] processed 30/136\n",
      "[INFO] processed 40/136\n",
      "[INFO] processed 50/136\n",
      "[INFO] processed 60/136\n",
      "[INFO] processed 70/136\n",
      "[INFO] processed 80/136\n",
      "[INFO] processed 90/136\n",
      "[INFO] processed 100/136\n",
      "[INFO] processed 110/136\n",
      "[INFO] processed 120/136\n",
      "[INFO] processed 130/136\n",
      ") dataset processing done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "features,labels = load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6fc8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(features,name):\n",
    "  with open(name, 'wb') as file:\n",
    "     pickle.dump(features, file)\n",
    "\n",
    "save_model(features,\"letters_model2.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9575a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_string_map = dict(zip(map(tuple, features), labels))\n",
    "# save_model(feature_string_map,\"letters_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2ca48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "  with open(name, 'rb') as file:\n",
    "      loaded_array_list = pickle.load(file)\n",
    "      return loaded_array_list\n",
    "features = load_model(\"letters_model2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9973f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_from_digit(digit):\n",
    "    char = {\n",
    "        0: 'B',\n",
    "        1: 'E',\n",
    "        2: 'H',\n",
    "        3: 'Y',\n",
    "        4: 'XOR',\n",
    "        5: 'AND',\n",
    "        6: 'NOT',\n",
    "        7: 'OR',\n",
    "        8: '(',\n",
    "        9: ')'\n",
    "    }\n",
    "    return char[digit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7ed9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(feat1: np.ndarray, feat2: np.ndarray) -> float:\n",
    "\n",
    "    return np.mean((feat1 - feat2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "625927fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR\n",
      "OR\n",
      "OR\n",
      "(\n",
      "NOT\n",
      "OR\n",
      "OR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def natural_sort_key(s):\n",
    "    \"\"\"Key function for natural sorting.\"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "output_file_path = 'results.txt'\n",
    "path_to_testset = r'testset3'\n",
    "filenames = sorted(os.listdir(path_to_testset), key=natural_sort_key)\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for i, fn in enumerate(filenames):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "        image=io.imread(os.path.join(path_to_testset, fn))\n",
    "        feature_extract,_=extract_hog_features(image)\n",
    "        \n",
    "        distances: float = [\n",
    "        calculate_distance(feature_extract, src_feat[:-1]) \n",
    "        for src_feat in features\n",
    "        ]\n",
    "        min_distance_index = np.argmin(distances)\n",
    "        # k = 5\n",
    "        # min_distance_indices = np.argpartition(distances, k)[:k]\n",
    "        # result = []\n",
    "        # for i, idx in enumerate(min_distance_indices):\n",
    "        #     result.append(get_char_from_digit(features[idx][-1]))\n",
    "        result=get_char_from_digit(features[min_distance_index][-1])\n",
    "        print(result)\n",
    "        #output_file.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
