{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.fft import fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_a_dataset = r'letters_data_set/A'\n",
    "path_to_b_dataset = r'letters_data_set/B'\n",
    "path_to_c_dataset = r'letters_data_set/C'\n",
    "path_to_d_dataset = r'letters_data_set/D'\n",
    "path_to_e_dataset = r'letters_data_set/E'\n",
    "path_to_f_dataset = r'letters_data_set/F'\n",
    "random_seed=2\n",
    "NN = KNeighborsClassifier(n_neighbors=15,weights='distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chain_code(img):\n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "   \n",
    "    directions = [0,1,2,3] \n",
    "    dx = [0, 1, 0, -1] \n",
    "    dy = [-1, 0, 1, 0]\n",
    "    chain_code = []\n",
    "\n",
    "\n",
    "# Iterate through each contour\n",
    "    for cnt in contours:\n",
    "\n",
    "        # Get coordinates of contour points\n",
    "        contour = cnt.reshape(-1,2) \n",
    "        \n",
    "        # Start from first point\n",
    "        x = contour[0][0]\n",
    "        y = contour[0][1]\n",
    "        \n",
    "        # Extract chain code  \n",
    "        max_iterations = 1000\n",
    "        while True:\n",
    "\n",
    "            # Check neighbor points clockwise\n",
    "            for d in directions:\n",
    "            \n",
    "                new_x = x + dx[d] \n",
    "                new_y = y + dy[d]  \n",
    "                \n",
    "                if new_x in contour[:,0] and new_y in contour[:,1]:\n",
    "                \n",
    "                    chain_code.append(d)\n",
    "                    x, y = new_x, new_y\n",
    "                    break\n",
    "            if len(chain_code) > max_iterations:\n",
    "               break\n",
    "\n",
    "    \n",
    "            if x == contour[0][0] and y == contour[0][1]:\n",
    "                break\n",
    "    #fft_codes = fft(chain_code) \n",
    "    #fft_magnitudes = np.abs(fft_codes) \n",
    "    #fft_features = fft_magnitudes / len(chain_code)\n",
    "    return chain_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    \n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    \n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9  \n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins,2)\n",
    "    h = hog.compute(img)\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_filenames_a = os.listdir(path_to_a_dataset)\n",
    "    img_filenames_b = os.listdir(path_to_b_dataset)\n",
    "    img_filenames_c = os.listdir(path_to_c_dataset)\n",
    "    img_filenames_d = os.listdir(path_to_d_dataset)\n",
    "    img_filenames_e = os.listdir(path_to_e_dataset)\n",
    "    img_filenames_f = os.listdir(path_to_f_dataset)\n",
    "\n",
    "\n",
    "    for i, fn in enumerate(img_filenames_a):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 'A'\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_a_dataset, fn)\n",
    "        img = cv2.imread(path)\n",
    "        features.append(extract_chain_code(img))\n",
    "        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_a)))\n",
    "    print(\"A dataset processing done\")\n",
    "    for i, fn in enumerate(img_filenames_b):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 'B'\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_b_dataset, fn)\n",
    "        img = cv2.imread(path)\n",
    "        features.append(extract_chain_code(img))\n",
    "        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_b)))     \n",
    "    print(\"B dataset processing done\")   \n",
    "    \n",
    "    for i, fn in enumerate(img_filenames_c):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "\n",
    "        label = 'C'\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_c_dataset, fn)\n",
    "        img = cv2.imread(path)\n",
    "        features.append(extract_chain_code(img))\n",
    "        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_c)))     \n",
    "    print(\"C dataset processing done\")   \n",
    "    \n",
    "    for i, fn in enumerate(img_filenames_d):\n",
    "        if fn.split('.')[-1] != 'png':\n",
    "            continue\n",
    "\n",
    "        label = 'D'\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_d_dataset, fn)\n",
    "        img = cv2.imread(path)\n",
    "        features.append(extract_chain_code(img))\n",
    "        \n",
    "        # show an update every 10 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_d)))     \n",
    "    print(\"D dataset processing done\")   \n",
    "    \n",
    "    # for i, fn in enumerate(img_filenames_e):\n",
    "    #     if fn.split('.')[-1] != 'png':\n",
    "    #         continue\n",
    "\n",
    "    #     label = 'E'\n",
    "    #     labels.append(label)\n",
    "\n",
    "    #     path = os.path.join(path_to_e_dataset, fn)\n",
    "    #     img = cv2.imread(path)\n",
    "    #     features.append(extract_chain_code(img))\n",
    "        \n",
    "    #     # show an update every 10 images\n",
    "    #     if i > 0 and i % 10 == 0:\n",
    "    #         print(\"[INFO] processed {}/{}\".format(i, len(img_filenames_e)))     \n",
    "    # print(\"E dataset processing done\")   \n",
    "    \n",
    " \n",
    "        \n",
    "    return features, labels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_features(features, max_length):\n",
    "    processed_features = []\n",
    "\n",
    "    for feature in features:\n",
    "        # Convert scalar to 1D array\n",
    "        feature = np.atleast_1d(feature)\n",
    "\n",
    "        # Flatten the feature\n",
    "        flat_feature = feature.flatten()\n",
    "\n",
    "        # Pad or truncate to the desired length\n",
    "        if len(flat_feature) < max_length:\n",
    "            flat_feature = np.pad(flat_feature, (0, max_length - len(flat_feature)))\n",
    "        elif len(flat_feature) > max_length:\n",
    "            flat_feature = flat_feature[:max_length]\n",
    "\n",
    "        processed_features.append(flat_feature)\n",
    "\n",
    "    return np.array(processed_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # Load dataset with extracted features\n",
    "    print('Loading dataset. This will take time ...')\n",
    "    features, labels = load_dataset()\n",
    "    print('Finished loading dataset.')\n",
    "    print(labels)\n",
    "    features = preprocess_features(features,1000)\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.3, random_state=random_seed)\n",
    "    \n",
    "    #print(labels)\n",
    "       \n",
    "    NN.fit(train_features, train_labels)\n",
    "        \n",
    "    \n",
    "    accuracy = NN.score(test_features, test_labels)\n",
    "        \n",
    "    print('accuracy: ', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] processed 10/45\n",
      "[INFO] processed 20/45\n",
      "[INFO] processed 30/45\n",
      "[INFO] processed 40/45\n",
      "A dataset processing done\n",
      "[INFO] processed 10/45\n",
      "[INFO] processed 20/45\n",
      "[INFO] processed 30/45\n",
      "[INFO] processed 40/45\n",
      "B dataset processing done\n",
      "[INFO] processed 10/45\n",
      "[INFO] processed 20/45\n",
      "[INFO] processed 30/45\n",
      "[INFO] processed 40/45\n",
      "C dataset processing done\n",
      "[INFO] processed 10/45\n",
      "[INFO] processed 20/45\n",
      "[INFO] processed 30/45\n",
      "[INFO] processed 40/45\n",
      "D dataset processing done\n",
      "Finished loading dataset.\n",
      "['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "accuracy:  38.88888888888889 %\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B']\n",
      "test0.png A\n",
      "\n",
      "['B']\n",
      "test1.png A\n",
      "\n",
      "['D']\n",
      "test2.png A\n",
      "\n",
      "['C']\n",
      "test3.png A\n",
      "\n",
      "['C']\n",
      "test4.png A\n",
      "\n",
      "['D']\n",
      "test5.png A\n",
      "\n",
      "['D']\n",
      "test6.png A\n",
      "\n",
      "['D']\n",
      "test7.png A\n",
      "\n",
      "['D']\n",
      "test8.png A\n",
      "\n",
      "['A']\n",
      "test192.png A\n",
      "\n",
      "['D']\n",
      "test193.png A\n",
      "\n",
      "['D']\n",
      "test194.png A\n",
      "\n",
      "['A']\n",
      "test195.png A\n",
      "\n",
      "['B']\n",
      "test196.png A\n",
      "\n",
      "['B']\n",
      "test197.png A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "letters=['A','B','C','D','E','F']\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Key function for natural sorting.\"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "output_file_path = 'results.txt'\n",
    "path_to_testset = r'testset3'\n",
    "filenames = sorted(os.listdir(path_to_testset), key=natural_sort_key)\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for i, fn in enumerate(filenames):\n",
    "        if fn.split('.')[-1] != 'png' and fn.split('.')[-1]!='jpg':\n",
    "            continue\n",
    "        im=cv2.imread(os.path.join(path_to_testset, fn,))\n",
    "        #im=cv2.adaptiveThreshold(im,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,75,15)\n",
    "        cv2.imshow(\"bounded Literals\", im)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        features = extract_chain_code(im)\n",
    "        features = preprocess_features([features], 1000)\n",
    "        pred = NN.predict(features)\n",
    "        result = f\"{fn} {letters[np.argmax(pred)]}\\n\"\n",
    "        print(pred)\n",
    "        print(result)  \n",
    "        output_file.write(result)\n",
    "# features=extract_hog_features(cv2.imread('test.png'))\n",
    "# pred=KNN.predict_proba([features])\n",
    "# print(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
